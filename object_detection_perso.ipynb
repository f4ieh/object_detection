{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/f4ieh/object_detection/blob/main/object_detection_perso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3192ca5c",
      "metadata": {
        "id": "3192ca5c"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "<h1 style = \"text-align:center\" > Détection d'objets </h1>\n",
        "<h2 style = \"text-align:center\" > Open-Images et fine-tuning </h2>\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "\n",
        "> Les exercices précédents étaient destinés à présenter le fonctionnement de quelques modèles d'objet détection, ainsi que l'inférence d'un modèle de détection d'objet. Intéressons nous maintenant à un vrai jeu de données de détection d'objets, ainsi que l'entraînement d'un modèle pré-entraîné.\n",
        ">\n",
        "> La thématique sera la détection des personnes ainsi que leur visage.\n",
        ">\n",
        "><img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/notebooks/object_detection_sample_pred.png\" style=\"height:500px\">\n",
        "\n",
        "### Structure de l'exercice\n",
        "\n",
        ">**1. Chargement des données** : utilisation de l'API fiftyone, exportation des données, format CoCo.\n",
        ">\n",
        ">\n",
        ">**2. Préparation des données** : transformer et normaliser les données pour les modèles de tensorflow, générateur de données personnalisé.\n",
        ">\n",
        ">\n",
        "> **3. Modélisation** : Importer et configurer un modèle de l'API de détection d'objets de tensorflow, Fine tune, évaluation du modèle.\n",
        "\n",
        "### Utiliser une marchine avec GPU\n",
        "\n",
        ">* Aller dans Edit/Notebook setting.\n",
        ">\n",
        "><img src='https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/colab_gpu_01.png'>\n",
        ">\n",
        "> * Vous pouvez ensuite choisir si vous désirez utiliser un GPU. Appuyer sur `SAVE` pour valider votre choix.\n",
        ">\n",
        "><img src='https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/colab_gpu_02.png'>\n",
        ">\n",
        "> Attention vous allez être basculer sur une nouvelle machine, et vous ne pouvez pas dépasser 12h/jour d'utilisation.\n",
        "\n",
        "⚠ ⚠ ⚠ ```\n",
        "Attention les cellules suivantes peuvent prendre quelques minutes pour s'exécuter. Et, activer bien \n",
        "``` ⚠ ⚠ ⚠\n",
        "\n",
        "* **(a)** Exécuter les cellules suivantes pour installer les packages nécessaires."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "metadata": {
        "id": "dPlAw9v0RH1B"
      },
      "id": "dPlAw9v0RH1B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "metadata": {
        "id": "h3Ot66Kx73__"
      },
      "id": "h3Ot66Kx73__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install right version tensorflow and fiftyone\n",
        "!pip install tensorflow==2.7.0\n",
        "!pip install opencv-python-headless==4.5.4.60 fiftyone\n",
        "!pip install fiftyone --no-binary fiftyone,voxel51-eta\n",
        "# Restart the runtime\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yStuNwAi74RI",
        "outputId": "ec0f3e61-2635-4dac-8b59-f3d6180f94c8"
      },
      "id": "yStuNwAi74RI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.7.0%2Bzzzcolab20220506150900-cp37-cp37m-linux_x86_64.whl (665.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 665.5 MB 23 kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.47.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.1.0)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (14.0.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (4.1.1)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.9.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.2.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.37.1)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.21.6)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.12)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.19.4)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.26.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.28.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.0)\n",
            "Installing collected packages: tensorflow-estimator, keras, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.1\n",
            "    Uninstalling tensorflow-2.9.1:\n",
            "      Successfully uninstalled tensorflow-2.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠ ⚠ ⚠ ```\n",
        "Attention, comme une réinitialisation du kernel est faite, il est nécessaire d'attendre l'execution avant de lancer une autre cellule de code.\n",
        "``` ⚠ ⚠ ⚠\n",
        "\n",
        "\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "<h2 style = \"text-align:center\" > 1. Chargement des données </h2>\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "\n",
        "## Jeu de données Open-Image\n",
        "\n",
        "> Pour rappel, [Open Images](https://storage.googleapis.com/openimages/web/index.html) comporte 8M d'images de toutes sortes avec les annotations de l'emplacement des objets. Des informations sur la segmentation sont également présentes pour une partie d'entre elles.\n",
        ">\n",
        "> Les auteurs du jeu de données mettent à disposition une API **[fiftyone](http://fiftyone.ai/)** ainsi qu'un outil avancé de visualisation.\n",
        ">\n",
        "> La fonction `load_zoo_dataset` de **`fiftyone.zoo`** ne contient pas que le jeu de données 'open-images-v6'.Il permet aussi de récupérer et piocher dans les jeux de données les plus célèbre.\n",
        "\n",
        "* **(b)** Exécuter la cellule suivante pour afficher les jeux de données disponibles depuis l'API."
      ],
      "metadata": {
        "id": "pS33NgUj7taD"
      },
      "id": "pS33NgUj7taD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfaf5ac5",
      "metadata": {
        "id": "cfaf5ac5"
      },
      "outputs": [],
      "source": [
        "import fiftyone.zoo as foz\n",
        "# List available zoo datasets\n",
        "print(foz.list_zoo_datasets())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c47a3a45",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "c47a3a45"
      },
      "source": [
        "> La fonction `load_zoo_dataset` permet alors :\n",
        ">\n",
        "> * **Sélectionner votre jeu de données** parmis la liste précédement affichée.\n",
        ">\n",
        ">\n",
        "> * Choisir le **split du jeu de données** : {'train', 'test', 'validation}\n",
        ">\n",
        ">\n",
        "> * Choisir le **type de label souhaité** : {'classifications', 'detections', 'relationships', 'segmentations'}\n",
        ">\n",
        ">\n",
        "> * Choisir les **classes d'objets** que contiennent les images (chat, chien...). L'[outil d'exploration](https://storage.googleapis.com/openimages/web/visualizer/index.html?set=train&type=detection&c=%2Fm%2F02p0tk3) d'open-image permet de se faire une idée.\n",
        ">\n",
        ">\n",
        "> * **Limiter le nombre d'échantillon** à charger.\n",
        ">\n",
        ">\n",
        "> ```python\n",
        ">dataset = fiftyone.zoo.load_zoo_dataset(\n",
        "              \"open-images-v6\",                           # Name of dataset\n",
        "              split=\"validation\",                         # Split of the dataset\n",
        "              label_types=[\"detections\", \"segmentations\"], # label types\n",
        "              classes=[\"Cat\", \"Dog\"],       # Classe of objects\n",
        "              max_samples=100,                            # Max smaples\n",
        "              drop_existing_dataset=True                  # Drop the name of existing dataset\n",
        "          )\n",
        ">```\n",
        "\n",
        "* **(c)** Charger sous le nom **`dataset`** un jeu de données provenant de **`'open-images-v6'`**, sur le split **`'validation'`**, avec uniquement le label **`'detections'`**, sur les classes **`[\"Human head\", \"Human body\"]`**, avec une limite de **`1000`** de samples, et **enlever les datasets existants**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone\n",
        "# Insérer votre code ici\n",
        "\n"
      ],
      "metadata": {
        "id": "Eg2QpFPGV-kD"
      },
      "id": "Eg2QpFPGV-kD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fc9f5ba",
      "metadata": {
        "id": "6fc9f5ba"
      },
      "outputs": [],
      "source": [
        "import fiftyone\n",
        "\n",
        "\n",
        "# Load the COCO-2017 validation split into a FiftyOne dataset\n",
        "dataset = fiftyone.zoo.load_zoo_dataset(\n",
        "              \"open-images-v6\",                           # Name of dataset\n",
        "              split=\"train\",                         # Split of the dataset\n",
        "              label_types=[\"detections\"],# label types\n",
        "              classes=[\"Football helmet\"],                     # Classe of objects , \"Human body\"\n",
        "              max_samples=1000,                            # Max smaples\n",
        "              drop_existing_dataset=True                  # Drop the name of existing dataset\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Un dataset provenant de fiftyone est composé de `Sample` : chacun correspond à une image ainsi que les informations sur l'annotation.\n",
        ">\n",
        "> Il est possible d'itérer pour les récupérer : \n",
        ">\n",
        "> ```python\n",
        ">for sample in dataset :\n",
        ">    [...]\n",
        ">```\n",
        ">\n",
        ">Il existe aussi la méthode `first` permettant de récupérer le premier `Sample` du jeu de données.\n",
        "\n",
        "* **(d)** Afficher le premier Sample du jeu de données **`dataset`**."
      ],
      "metadata": {
        "id": "tbD3nGt18htP"
      },
      "id": "tbD3nGt18htP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Insérer votre code ici\n",
        "\n"
      ],
      "metadata": {
        "id": "bA_8CD1KWFUA"
      },
      "id": "bA_8CD1KWFUA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a77bea11",
      "metadata": {
        "id": "a77bea11"
      },
      "outputs": [],
      "source": [
        "dataset.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2e9597b",
      "metadata": {
        "id": "d2e9597b"
      },
      "source": [
        "> Les objets `Sample` peuvent être manipulés comme des dictionnaires. Il est alors facile de récupérer le chemin vers l'image ainsi que les objets labélisés.\n",
        ">\n",
        ">\n",
        "> Ce format de données est particulié à l'API **`fiftyone`**, c'est pourquoi nous allons choisir ici de les convertir en **format CoCo**, le format le plus couramment utilisé pour stocker les labels en détection d'objets.\n",
        ">\n",
        "> Pour cela, la méthode `export` de l'objet **`dataset`** exporte les images ainsi que stocke la labélisation dans un fichier json.\n",
        ">\n",
        ">```python\n",
        ">dataset.export(\n",
        ">    export_dir=folder_export, # Folder export data\n",
        ">    dataset_type=fiftyone.types.COCODetectionDataset # Format of label file\n",
        ">)\n",
        ">```\n",
        "> Les données seront exportées dans le folder_export sous la structure suivante : \n",
        ">\n",
        ">```\n",
        "│─── folder_export\n",
        "│   └───data\n",
        "│       │   7166544280_9d975c4d9a_n.jpg\n",
        "│       │   6958243974_8851425ddb_n.jpg\n",
        "│       │   8729501081_b993185542_m.jpg\n",
        "│       │    ...\n",
        "│   └───labels.csv\n",
        ">```\n",
        "\n",
        "* **(e)** Exécuter la cellule suivante pour exporter les données dans le dossier *dataset_train*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf24f816",
      "metadata": {
        "id": "cf24f816"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "folder_data = \"./dataset_train/\"\n",
        "# Export data to CoCo format\n",
        "dataset.export(\n",
        "    export_dir=folder_data,\n",
        "    dataset_type=fiftyone.types.COCODetectionDataset\n",
        ")\n",
        "# Show elements in the folder data\n",
        "os.listdir(folder_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Données format COCO\n",
        "\n",
        "> Les données sous format COCO sont stockées dans un format **`json`**, autrement dit sous format d'un dictionnaire. Le dictionnaire est composé des clés suivantes :\n"
      ],
      "metadata": {
        "id": "CkGY-1Ai-Dx0"
      },
      "id": "CkGY-1Ai-Dx0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aea69374",
      "metadata": {
        "scrolled": true,
        "id": "aea69374"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open( folder_data+\"/labels.json\" , \"r\" ) as f: \n",
        "    data = json.load(f)\n",
        "    \n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Avec **`info`**, correspondant aux informations sur le jeu de données (date de création, auteur, version, description...)."
      ],
      "metadata": {
        "id": "qVIYBBINCnhq"
      },
      "id": "qVIYBBINCnhq"
    },
    {
      "cell_type": "code",
      "source": [
        "data['info']"
      ],
      "metadata": {
        "id": "WuSA-75kCnOm"
      },
      "id": "WuSA-75kCnOm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "80cccb7b",
      "metadata": {
        "id": "80cccb7b"
      },
      "source": [
        "> **`licenses`** : informations sur la licence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98481f35",
      "metadata": {
        "id": "98481f35"
      },
      "outputs": [],
      "source": [
        "data['licenses']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**`categories`** : relation entre index de classes et le nom de classes."
      ],
      "metadata": {
        "id": "xPCBeAWxCtty"
      },
      "id": "xPCBeAWxCtty"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a5163b0",
      "metadata": {
        "scrolled": true,
        "id": "4a5163b0"
      },
      "outputs": [],
      "source": [
        "data[\"categories\"][:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**`images`** : Informations sur chaque image du jeu de données (nom de l'image, description, indice de l'image...)"
      ],
      "metadata": {
        "id": "cXggYnmOCyri"
      },
      "id": "cXggYnmOCyri"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d040e050",
      "metadata": {
        "id": "d040e050"
      },
      "outputs": [],
      "source": [
        "data[\"images\"][:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* **`annotations`** : Liste d'objets annotés\n",
        "\n"
      ],
      "metadata": {
        "id": "PxO4w7sTDO6T"
      },
      "id": "PxO4w7sTDO6T"
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"annotations\"][:2]"
      ],
      "metadata": {
        "id": "L9WtyeXvDxj6"
      },
      "id": "L9WtyeXvDxj6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Maintenant que les données ont été exportées, préparons les.\n",
        "\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "<h2 style = \"text-align:center\" > 2. Préparation des données </h2>\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "\n",
        "> Pour faliciter la manipulation des données, nous allons convertir le dictionnaire \"annotations\" ainsi que \"images\" dans un dataframe **`pandas`**.\n",
        "\n",
        "* **(a)** Exécuter la cellule suivante pour les convertir ainsi que fusionner les deux dataframes."
      ],
      "metadata": {
        "id": "C3_zTHB-F7eu"
      },
      "id": "C3_zTHB-F7eu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16369bc3",
      "metadata": {
        "id": "16369bc3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data[\"annotations\"])\n",
        "df_im = pd.DataFrame(data['images'])\n",
        "df = pd.merge(df_im, df, left_on=\"id\", right_on=\"image_id\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Voici un tableau avec signification des colones les plus importantes : \n",
        ">\n",
        ">|Variable | Description |\n",
        "| ----- | ----- |\n",
        "| **id_x** ou **image_id** | Identifiant de l'image|\n",
        "| **file_name** | Nom de l'image |\n",
        "| **height** | Hauteur en pixel de l'image |\n",
        "| **width** | Largeur en pixel de l'image |\n",
        "| **id_y** | Identification de l'objet |\n",
        "| **category_id** | Catégorie de l'objet (voiture, avion, personne...) |\n",
        "| **bbox** | Coordonnées $x_{min}$, $y_{min}$, $w$ et $h$ non normalisées |\n",
        ">\n",
        "> En effet, dans le format CoCo, la colonne **`bbox`** correspond aux coordonnées $x_{min}$, $y_{min}$, $w$ et $h$ non normalisées, c'est-à-dire dans l'echelle des pixels.\n",
        ">\n",
        "> <img src='https://assets-datascientest.s3.eu-west-1.amazonaws.com/notebooks/object_detection_coord_coco.png' style='width:300px'>\n",
        "><center> <b> Figure :</b> Format des annotations sur CoCo </center>\n",
        "><br></br>\n",
        ">\n",
        "> Alors que le format demandé par les modèles de tensorflow est le suivant : $y_{min}$, $x_{min}$, $y_{max}$ et $x_{max}$ normalisés.\n",
        ">\n",
        ">\n",
        ">\n",
        "> <img src='https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/object_detection_coord_min_max.png' style='width:300px'>\n",
        ">\n",
        "><center> <b> Figure :</b> Format des annotations sur Tensorflow </center>\n",
        "><br></br>\n",
        "\n",
        "* **(b)** Exécuter la cellule suivante pour convertir et normaliser la colonne **bbox**."
      ],
      "metadata": {
        "id": "06QgU1qhGUDv"
      },
      "id": "06QgU1qhGUDv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4649ccb",
      "metadata": {
        "id": "a4649ccb"
      },
      "outputs": [],
      "source": [
        "def convertWHXY_normalized(x):\n",
        "    bbox = x['bbox']\n",
        "    w = x['width']\n",
        "    h = x['height']\n",
        "    return [bbox[1]/h, bbox[0]/w, (bbox[1]+bbox[3])/h, (bbox[0]+bbox[2])/w]\n",
        "\n",
        "df['bbox_normilized'] = df[[\"height\", \"width\", \"bbox\"]].apply(convertWHXY_normalized, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Catégorie d'objet\n",
        "\n",
        "> Analysons maintenant les catégories disponibles dans notre jeu de données.\n",
        "\n",
        "* **(c)** Afficher le nombre de catégories d'objets différents."
      ],
      "metadata": {
        "id": "o_XdaEbFmsqi"
      },
      "id": "o_XdaEbFmsqi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Insérer votre code ici\n",
        "\n"
      ],
      "metadata": {
        "id": "mbf6mw6Tgkn8"
      },
      "id": "mbf6mw6Tgkn8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.category_id.value_counts()"
      ],
      "metadata": {
        "id": "mOMdtsKIoA60"
      },
      "id": "mOMdtsKIoA60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **(d)** D'après le dictionnaire de correspondance **`data['categories']`**, afficher le TOP10 des **noms d'objet** les plus présents."
      ],
      "metadata": {
        "id": "LjjguF7jpKYU"
      },
      "id": "LjjguF7jpKYU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Insérer votre code ici\n",
        "\n"
      ],
      "metadata": {
        "id": "QlL1E_E-e5l1"
      },
      "id": "QlL1E_E-e5l1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df['category_name'] = df['category_id'].apply(lambda x : data['categories'][x]['name'] )\n",
        "df['category_name'].value_counts()[:30].plot.bar()\n",
        "plt.title('Top 30 class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TKKcJNfooJ5P"
      },
      "id": "TKKcJNfooJ5P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Ici, nous allons faire le choix de nous intéresser qu'aux objets \"Human body\" (id:261) et \"Humain head\" (id:268).\n",
        "\n",
        "* **(e)** Exécuter la cellule suivante pour selectioner uniquement ces deux objets, et les remplacer sous l'indice 0 et 1."
      ],
      "metadata": {
        "id": "K6cc573gpa58"
      },
      "id": "K6cc573gpa58"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c965e6b4",
      "metadata": {
        "id": "c965e6b4"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = df[df.category_id.isin([34, 11])]\n",
        "df.category_id = df.category_id.replace([34, 11], [0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Dans l'état actuel, chaque observation correspond à une annotation. Dans la suite, nous allons avoir besoin d'aller dans la maille des images (une observation correpond aux informations d'une image).\n",
        "\n",
        "* **(f)** Exécuter la cellule suivante pour aggréger les données dans la maille des images."
      ],
      "metadata": {
        "id": "51qyHgHgqSZm"
      },
      "id": "51qyHgHgqSZm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9d0821",
      "metadata": {
        "id": "dc9d0821"
      },
      "outputs": [],
      "source": [
        "df_group = df.groupby(\"id_x\").agg({\"file_name\":\"min\", \"image_id\":\"count\", \"height\":\"min\", \"width\":\"min\", \"bbox_normilized\":list, \"category_id\":list})\n",
        "df_group"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **(g)** Exécuter la cellule suivante pour afficher une des images annontées."
      ],
      "metadata": {
        "id": "jw9SxIuUqv7k"
      },
      "id": "jw9SxIuUqv7k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "912b1b81",
      "metadata": {
        "id": "912b1b81"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "import matplotlib.pyplot as plt\n",
        "from object_detection.utils import label_map_util\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Define relation index/name object\n",
        "category_index = {0: {'id': 0, 'name': 'Person'}, 1: {'id': 1, 'name': 'Football helmet'}}\n",
        "# category_index = dict(list(zip(list(map(lambda x : x['id'], data['categories'])), data['categories'])))\n",
        "\n",
        "# Select a observation\n",
        "idx = 10\n",
        "observation = df_group.iloc[idx]\n",
        "\n",
        "# Load the image and convert it into RGB\n",
        "img = cv2.imread(folder_data+\"data/\"+observation[\"file_name\"])[..., ::-1]\n",
        "\n",
        "\n",
        "# Overwrite the image to add annotation\n",
        "image_np_with_detections = img.copy()\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_detections,\n",
        "      np.array(observation[\"bbox_normilized\"]),\n",
        "      np.array(observation[\"category_id\"]),\n",
        "      np.ones(len(observation[\"category_id\"])),\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      min_score_thresh=.30)\n",
        "\n",
        "# Show image annoted\n",
        "plt.figure(figsize=(15,20))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Le modèle de détection d'objets que nous allons charger/fine tune provient de l'API de tensorflow. La liste des modèles disponibles est disponible à partir ce [lien](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).\n",
        ">\n",
        "> Hélas, ce type de modèle **n'est pas doté** de méthode d'entraînement `fit` ou `fit_generator`. Il sera alors nécessaire de recourir aux boucles d'entraînement personnalisées.\n",
        ">\n",
        "> Le modèle aura besoin pour s'entraîner :\n",
        ">\n",
        "> * D'un **batch d'images** de forme : [nb_elements, dim_x, dim_y, 3]\n",
        ">\n",
        ">\n",
        ">* De la liste des **coordonnées de chaque objet** :\n",
        ">\n",
        "> ```python\n",
        ">[\n",
        "> # First image\n",
        "> [[0.14961833, 0.30982906, 0.7969466, 0.6880342], # coordinate object\n",
        ">  [0.1480916, 0.3974, 0.31603053, 0.5811966]], # coordinate object\n",
        "> # Seconde image\n",
        "> [[0.16666667, 0.0, 1.0, 0.178125], # coordinate object\n",
        ">  [0.34791, 0.13125, 0.87916666, 0.7515625], # coordinate object\n",
        ">  [0.31666666, 0.5890625, 0.5229167, 0.740625]] # coordinate object\n",
        ">...\n",
        ">]\n",
        ">```\n",
        ">\n",
        ">\n",
        "> * De la liste des **classes de chaque objets** sous format one hot  :\n",
        ">\n",
        "> ```python\n",
        ">[\n",
        "> # First image\n",
        "> [[1.0, 0.0], # class object\n",
        ">  [0.0, 1.0]], # class object\n",
        "> # Seconde image\n",
        "> [[1.0, 0.0], # class object\n",
        ">  [1.0, 0.0], # class object\n",
        ">  [0.0, 1.0]] # class object\n",
        ">...\n",
        ">]\n",
        ">```\n",
        ">\n",
        "> Comme la forme d'entrée peut varier en fonction du nombre d'objet par image, rendant les datasets de tensorflow plus difficile d'utilisation, nous allons faire le choix de définir un générateur personnalisé.\n",
        "\n",
        "* **(h)** Exécuter la cellule suivante pour définir notre générateur."
      ],
      "metadata": {
        "id": "LKT06CR2yfjN"
      },
      "id": "LKT06CR2yfjN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94835cf0",
      "metadata": {
        "id": "94835cf0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, df,\n",
        "                 batch_size,\n",
        "                 folder_image,\n",
        "                 nb_class = 2,\n",
        "                 input_size=(128, 128, 3),\n",
        "                 shuffle=True):\n",
        "        \n",
        "        self.df = df.copy()\n",
        "        self.batch_size = batch_size\n",
        "        self.folder_image = folder_image\n",
        "        self.nb_class = nb_class\n",
        "        self.input_size = input_size\n",
        "        self.shuffle = shuffle\n",
        "        self.n = len(self.df)\n",
        "        \n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.df = self.df.sample(frac=1)\n",
        "    \n",
        "    def __get_input(self, path, target_size):\n",
        "        # Load the image\n",
        "        image = tf.keras.preprocessing.image.load_img(self.folder_image + path)\n",
        "        # Convert to array\n",
        "        image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
        "        # resize it\n",
        "        image_arr = tf.image.resize(image_arr, (target_size[0], target_size[1])).numpy()\n",
        "        \n",
        "        return image_arr\n",
        "    \n",
        "    def __get_output(self, label, num_classes):\n",
        "        return tf.keras.utils.to_categorical(label, num_classes=num_classes)\n",
        "    \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Select the batch of data\n",
        "        batches = self.df.iloc[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "\n",
        "        X_batch = []\n",
        "        # Load images  \n",
        "        for path in batches[\"file_name\"] :\n",
        "            im = self.__get_input(path, target_size=self.input_size)\n",
        "            X_batch.append(im)\n",
        "\n",
        "        # Coordinate label\n",
        "        gt_box_tensors =[gt_box_np for gt_box_np in batches['bbox_normilized']]\n",
        "\n",
        "        # Class Label\n",
        "        gt_classes_one_hot_tensors = [tf.one_hot(gt_label_np, self.nb_class).numpy() for gt_label_np in batches['category_id']]\n",
        "    \n",
        "        return np.array(X_batch), gt_box_tensors, gt_classes_one_hot_tensors\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n // self.batch_size\n",
        "    \n",
        "batch_size = 8\n",
        "dataTrain = DataGenerator(df_group, batch_size=batch_size, folder_image=folder_data+\"data/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "<h2 style = \"text-align:center\" > 3. Modélisation </h2>\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "\n",
        "\n",
        "> Pour rappel, l'API de tensorflow propose une liste exhaustive de modèles pré-entraînés : https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n",
        ">\n",
        ">\n",
        "> <img src='https://assets-datascientest.s3.eu-west-1.amazonaws.com/notebooks/object_detection_dl_model.png' style='width:300px'>\n",
        "><center> <b> Figure :</b> Liste des modèles disponibles sur l'<a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\"> API de tensorflow</a> </center>\n",
        ">\n",
        ">\n",
        "> Les colonnes du tableau correspondent : \n",
        ">\n",
        "> * **Model name** : Indicant le type de modèles utilisés (Yolo, R-CNN, SSD...), le backbone utilisé et la taille d'entrée des données. Le lien permet de télécharger le modèle.\n",
        ">\n",
        "> * **Speed** : Vitesse d'inférence sur une machine spécifique.\n",
        ">\n",
        ">\n",
        ">* **COCO mAP** : Performance du modèle sur le jeu de donnée COCO.\n",
        ">\n",
        ">\n",
        ">* **Outputs** : Type de prédiction du modèle.\n",
        ">\n",
        ">\n",
        "> Pour utiliser le modèle, il sera nécessaire de mettre le checkpoint du modèle dans le dossier *models/research/object_detection/test_data*.\n",
        "\n",
        "* **(a)** Exécuter la cellule suivante pour télécharger un des modèles, décompresser et placer le checkpoint dans le bon dossier."
      ],
      "metadata": {
        "id": "1Y_WbsT69Yu6"
      },
      "id": "1Y_WbsT69Yu6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9acadf26",
      "metadata": {
        "id": "9acadf26"
      },
      "outputs": [],
      "source": [
        "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Pour facilité l'utisation des modèles, l'api de détection d'objet de tensorflow utilise des **pipelines** contenant : le **prétraitement des données**, un système d'**augmenatation de données**, le **modèle** et les **fonctions de pertes**, ainsi que le **post traitement** avec la non max suppression.\n",
        ">\n",
        "> Les configurations des modèles se trouvent dans le dossier \"models/research/object_detection/configs/tf2/\".\n",
        "\n",
        "* **(b)** Exécuter la cellule suivante pour charger le fichier config de notre modèle téléchagé plus haut."
      ],
      "metadata": {
        "id": "qtfJ9r7jm4a3"
      },
      "id": "qtfJ9r7jm4a3"
    },
    {
      "cell_type": "code",
      "source": [
        "from object_detection.utils import config_util\n",
        "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
        "# Load the config model, and override it to adapt it on the case\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "configs"
      ],
      "metadata": {
        "id": "SyrNRNTFnHPE"
      },
      "id": "SyrNRNTFnHPE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Il est très facile de changer le fichier config pour l'adapter à notre tâche.\n",
        "\n",
        "* **(c)** Exécuter la cellule suivante pour changer quelques éléments dans le dictionnaire de config."
      ],
      "metadata": {
        "id": "9EqDB-N9qOm3"
      },
      "id": "9EqDB-N9qOm3"
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 2\n",
        "\n",
        "# Override it to adapt it on the case\n",
        "model_config = configs['model']\n",
        "# Override the numbrer of class\n",
        "model_config.ssd.num_classes = num_classes\n",
        "# Freeze the batchnorm layer in fine tuning.\n",
        "model_config.ssd.freeze_batchnorm = True"
      ],
      "metadata": {
        "id": "15M4JcM2qPc5"
      },
      "id": "15M4JcM2qPc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> La fonction `build` de **`model_builder`** permet de construire le modèle en fonction fichier **`model_config`**.\n",
        "\n",
        "* **(d)** Exécuter la cellule suivante pour constuire notre modèle sous le nom **`detection_model`**."
      ],
      "metadata": {
        "id": "O8_F83NXu1wP"
      },
      "id": "O8_F83NXu1wP"
    },
    {
      "cell_type": "code",
      "source": [
        "from object_detection.builders import model_builder\n",
        "\n",
        "# Clear all state in tensorflow\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Build the model\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=True)"
      ],
      "metadata": {
        "id": "_kLpN1hexSeb"
      },
      "id": "_kLpN1hexSeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> La méthode [`Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) de **`tensorflow`** gère la sauvegarde/restoration des variables d'un modèle. Il existe un ensemble d'argument pour renseigner les bonnes correspondances entre les variables.\n",
        ">\n",
        ">```python\n",
        "ckpt = tf.compat.v2.train.Checkpoint(\n",
        "      _base_tower_layers_for_heads = detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
        "    )\n",
        "```\n",
        ">\n",
        ">\n",
        "> Une fois le Checkpoint bien défini, la méthode `restore` du Checkpoint fait le lien et restore les variables du modèle. Ajouter une méthode `expect_partial`permet de cacher les avertissements correspondants à la restaurations de point de contrôle incomplètes.\n",
        ">```python\n",
        "ckpt.restore(checkpoint_path).expect_partial()\n",
        "```\n",
        "\n",
        "* **(e)** Exécuter la cellule suivante pour restorer nos poids de modèles. Nous allons faire le choix de ne pas restorer la tête correspondant à la classification des objets."
      ],
      "metadata": {
        "id": "6O2GFRb31u_g"
      },
      "id": "6O2GFRb31u_g"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Restoring weights for fine-tuning...', flush=True)\n",
        "\n",
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "\n",
        "# Checkpoint for box predictor\n",
        "ckpt_box_predictor = tf.compat.v2.train.Checkpoint(\n",
        "    # Restore the base head\n",
        "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    # Restore the classification head\n",
        "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
        "    # Restore the regression head\n",
        "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
        "    )\n",
        "\n",
        "# Checkpoint for the model\n",
        "ckpt_model = tf.compat.v2.train.Checkpoint(\n",
        "          # Restore the backbone\n",
        "          _feature_extractor=detection_model._feature_extractor,\n",
        "          # Box predictor checkpoint\n",
        "          _box_predictor=ckpt_box_predictor)\n",
        "\n",
        "# Checkpoint for the final model\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=ckpt_model)\n",
        "\n",
        "# Restore the weight of the model\n",
        "ckpt.restore(checkpoint_path).expect_partial()\n",
        "\n",
        "\n",
        "# Run model through a dummy image so that variables are created\n",
        "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
        "prediction_dict = detection_model.predict(image, shapes)\n",
        "_ = detection_model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "print('Weights restored!')"
      ],
      "metadata": {
        "id": "aWue4oG55t_u"
      },
      "id": "aWue4oG55t_u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Maintenant que le modèle est chargé, entrainons le sur notre tâche.\n",
        "\n",
        "### Variables à Fine tune\n",
        "\n",
        "> Pour fine tune un modèle pré-entraîné, il est coutume de freeze les couches d'extraction de caractéristiques (backbone...).\n",
        ">\n",
        "> C'est pourquoi, nous allons faire le choix d'entraîner uniquement la tête de détection ainsi que la tête de classification.\n",
        "\n",
        "\n",
        "* **(f)** Exécuter la cellule suivante pour stocker toutes les variables à entraîner dans la liste **`to_fine_tune`**."
      ],
      "metadata": {
        "id": "DtBVTVQlCsUU"
      },
      "id": "DtBVTVQlCsUU"
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.set_learning_phase(True)\n",
        "\n",
        "# Select trainable variables.\n",
        "trainable_variables = detection_model.trainable_variables\n",
        "\n",
        "prefixes_to_train = [\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
        "\n",
        "to_fine_tune = []\n",
        "# Select each variable with a prefixe from prefixes_to_train\n",
        "for var in trainable_variables:\n",
        "    if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "        to_fine_tune.append(var)"
      ],
      "metadata": {
        "id": "68JDlBBeCmzb"
      },
      "id": "68JDlBBeCmzb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fonction d'entraînement\n",
        "\n",
        ">  Pour rappel, tensorflow permet de personaliser tout le processus d'entraînement :\n",
        ">\n",
        ">```python\n",
        "># Record every operation in GradientTape\n",
        ">with tf.GradientTape() as tape:\n",
        ">    # Model prediction.\n",
        ">    y_pred = model(X)\n",
        ">    # Compute the loss function.\n",
        ">    loss_value = loss(y_true, y_pred)\n",
        ">\n",
        "># Compute the gradient function thanks to GradientTape\n",
        ">grads = tape.gradient(loss_value, parameters_to_train)\n",
        ">\n",
        "># Update the weights of the model.\n",
        ">optimizer.apply_gradients(zip(grads, parameters_to_train))\n",
        ">```\n",
        ">\n",
        ">Ici, commme **`detection_model`** est un pipeline, il est dôté des méthodes suviantes : \n",
        ">\n",
        "> * `preprocess` : Normalise des images du lot de données.\n",
        ">\n",
        ">\n",
        "> * `predict` : Prédit sour le format de dictionnaire les sorties du modèle\n",
        ">\n",
        ">\n",
        "> * `provide_groundtruth` : Fournit la vraie cible au modèle. Indispensable pour calculer la fonction de perte `loss`.\n",
        ">\n",
        ">\n",
        "> * `loss` : Prédit sous le format de dictionnaire les valeurs des différentes fonctions de perte renseignées dans le fichier config. Ici, les valeurs des clés 'Loss/localization_loss' et 'Loss/classification_loss' correspondent respectivement à la fonction de perte de localisation et de classification.\n",
        "\n",
        "* **(g)** Exécuter la cellule suivante pour définir notre fonction d'entraînement"
      ],
      "metadata": {
        "id": "KdPH4dOqKqEf"
      },
      "id": "KdPH4dOqKqEf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d2cc09b",
      "metadata": {
        "id": "0d2cc09b"
      },
      "outputs": [],
      "source": [
        "# Set up forward + backward pass for a single train step.\n",
        "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
        "    \"\"\"Get a tf.function for training step.\"\"\"\n",
        "\n",
        "    # Use tf.function for a bit of speed.\n",
        "    # Comment out the tf.function decorator if you want the inside of the\n",
        "    # function to run eagerly.\n",
        "    # @tf.function\n",
        "    def train_step_fn(image_tensors,\n",
        "                    groundtruth_boxes_list,\n",
        "                    groundtruth_classes_list):\n",
        "        \"\"\"A single training iteration.\n",
        "\n",
        "        Args:\n",
        "          image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
        "            Note that the height and width can vary across images, as they are\n",
        "            reshaped within this function to be 640x640.\n",
        "          groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
        "            tf.float32 representing groundtruth boxes for each image in the batch.\n",
        "          groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
        "            with type tf.float32 representing groundtruth boxes for each image in\n",
        "            the batch.\n",
        "\n",
        "        Returns:\n",
        "          A scalar tensor representing the total loss for the input batch.\n",
        "        \"\"\"\n",
        "        # Shape of images\n",
        "        shapes = tf.constant(batch_size * [[128, 128, 3]], dtype=tf.int32)\n",
        "\n",
        "        \n",
        "\n",
        "        # Give true target  to the model\n",
        "        model.provide_groundtruth(\n",
        "            groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "            groundtruth_classes_list=groundtruth_classes_list)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            # Prepross input adaptative to different shape of images\n",
        "            preprocessed_images = tf.concat(\n",
        "              [detection_model.preprocess(tf.expand_dims(image_tensor,0))[0]\n",
        "               for image_tensor in image_tensors], axis=0)\n",
        "            # preprocessed_images = detection_model.preprocess(tf.convert_to_tensor(image_tensors, tf.float32))\n",
        "            # Predict values\n",
        "            prediction_dict = model.predict(preprocessed_images, shapes)\n",
        "            # Compute losses\n",
        "            losses_dict = model.loss(prediction_dict, shapes)\n",
        "            # Extract localization_loss and classification_loss\n",
        "            total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
        "\n",
        "        # Compute the gradient\n",
        "        gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
        "        # Apply Back propagation\n",
        "        optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "        return total_loss \n",
        "\n",
        "    return train_step_fn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Maintenant que le modèle est défini, chargé et que la fonction d'entraînement est défini, il ne reste plus qu'à entraîner le modèle.\n",
        "\n",
        "* **(h)** Exécuter la cellule suivante pour entraîner le modèle."
      ],
      "metadata": {
        "id": "3eRI-u9eQz9i"
      },
      "id": "3eRI-u9eQz9i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dd32bdc",
      "metadata": {
        "id": "5dd32bdc"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "EPOCHS = 150\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Training function\n",
        "train_step_fn = get_model_train_step_function(\n",
        "    detection_model, optimizer, to_fine_tune)\n",
        "\n",
        "# Train the model\n",
        "for i in range(EPOCHS):\n",
        "    for image_tensors, gt_boxes_list, gt_classes_list in tqdm(dataTrain) :\n",
        "        # Convert localisation of box to tensor\n",
        "        gt_boxes_list =[tf.convert_to_tensor(gt_box_np, dtype=tf.float32) for gt_box_np in gt_boxes_list]\n",
        "        # Convert class of box to tensor\n",
        "        gt_classes_list =[tf.convert_to_tensor(gt_class_np, dtype=tf.float32) for gt_class_np in gt_classes_list]\n",
        "        # Compute the training function\n",
        "        total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
        "    # Show the loss of the last batch\n",
        "    print('Epoch ' + str(i) + ', loss=' +  str(total_loss.numpy()),flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Le modèle est maintenant entraîné, il ne reste plus qu'à définir le processus d'inférence. Pour obtenir le même résultat qu'un modèle d'inférence, il sera nécessaire de :\n",
        "> * 1. **Prétraiter** l'image d'entrée : normalisation\n",
        ">\n",
        ">```python\n",
        ">preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        ">```\n",
        ">\n",
        ">\n",
        "> * 2. **Prédire les objets** dans l'image\n",
        ">\n",
        ">```python\n",
        ">prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        ">```\n",
        ">\n",
        ">\n",
        "> 3. **Post-traiter** la sortie du modèle : non maximum suppresion\n",
        ">\n",
        ">```python\n",
        ">prediction_final_dict = detection_model.postprocess(prediction_dict, shapes)\n",
        ">```\n",
        "\n",
        "* **(i)** Exécuter la cellule suivante pour définir la fonction `detection` ainsi qu'afficher la sortie pour une des images de notre jeu de données."
      ],
      "metadata": {
        "id": "K9VlHh-1RiWh"
      },
      "id": "K9VlHh-1RiWh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf41475f",
      "metadata": {
        "id": "cf41475f"
      },
      "outputs": [],
      "source": [
        "def detect(input_tensor):\n",
        "  \"\"\"Run detection on an input image.\n",
        "\n",
        "  Args:\n",
        "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
        "      Note that height and width can be anything since the image will be\n",
        "      immediately resized according to the needs of the model within this\n",
        "      function.\n",
        "\n",
        "  Returns:\n",
        "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
        "      and `detection_scores`).\n",
        "  \"\"\"\n",
        "  preprocessed_image, shapes = detection_model.preprocess(tf.convert_to_tensor(input_tensor, tf.float32))\n",
        "  prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "  return detection_model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "detect(np.expand_dims(img, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **(j)** Exécuter la cellule suivante pour afficher la prédiction du modèle."
      ],
      "metadata": {
        "id": "9QfDDPQNUBFP"
      },
      "id": "9QfDDPQNUBFP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538392f1",
      "metadata": {
        "id": "538392f1"
      },
      "outputs": [],
      "source": [
        "def show_img(img, detector, threshold=0.3):\n",
        "    detector_output = detector(np.expand_dims(img, axis=0))\n",
        "    detector_output = {key:value.numpy() for key,value in detector_output.items()}\n",
        "    \n",
        "    image_np_with_detections = img.copy()\n",
        "\n",
        "    # Use keypoints if available in detections\n",
        "    keypoints, keypoint_scores = None, None\n",
        "\n",
        "    if 'detection_keypoints' in detector_output:\n",
        "        keypoints = detector_output['detection_keypoints'][0]\n",
        "        keypoint_scores = detector_output['detection_keypoint_scores'][0]\n",
        "\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "          image_np_with_detections,\n",
        "          detector_output['detection_boxes'][0],\n",
        "          (detector_output['detection_classes'][0]+0).astype(int),\n",
        "          detector_output['detection_scores'][0],\n",
        "          category_index,\n",
        "          use_normalized_coordinates=True,\n",
        "          max_boxes_to_draw=50,\n",
        "          min_score_thresh=threshold,\n",
        "          agnostic_mode=False)\n",
        "    \n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.imshow(image_np_with_detections)\n",
        "    plt.show()\n",
        "    \n",
        "show_img(img, detect, threshold=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **(k)** Tester votre modèles sur d'autres images"
      ],
      "metadata": {
        "id": "UoGmslgcT5iU"
      },
      "id": "UoGmslgcT5iU"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, urllib\n",
        "\n",
        "def url_to_image(url):\n",
        "    resp = urllib.request.urlopen(url) \n",
        "    img = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    img = cv2.imdecode(img, -1)\n",
        "    img = img[..., [2,1,0]]\n",
        "    return img\n",
        "\n",
        "img2 = url_to_image(\"http://cdn.playbuzz.com/cdn/2361ba95-3be3-4f7d-b0a0-542155824490/495210c5-f896-41e2-8d27-3c093460c7a4.jpg\")\n",
        "show_img(img2, detect, threshold=0.3)"
      ],
      "metadata": {
        "id": "6X_Ql0S4uenT"
      },
      "id": "6X_Ql0S4uenT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img2 = url_to_image(\"https://thumbs.dreamstime.com/b/american-football-match-wolves-blue-dragon-belgrade-serbia-may-belgrade-belgrade-team-winner-50415289.jpg\")\n",
        "show_img(img2, detect, threshold=0.3)"
      ],
      "metadata": {
        "id": "oW9ZVXzp29Lv"
      },
      "id": "oW9ZVXzp29Lv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img2 = url_to_image(\"https://thumbs.dreamstime.com/b/american-football-match-wolves-blue-dragon-belgrade-serbia-may-belgrade-belgrade-team-winner-50415289.jpg\")\n",
        "show_img(img2, detect, threshold=0.4)"
      ],
      "metadata": {
        "id": "Ey1gphZwujRC"
      },
      "id": "Ey1gphZwujRC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_pipeline_proto = config_util.create_pipeline_proto_from_configs(configs)\n"
      ],
      "metadata": {
        "id": "G7uV9UCXSPFP"
      },
      "id": "G7uV9UCXSPFP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_util.save_pipeline_config(new_pipeline_proto, '/content/new_config')\n",
        "\n"
      ],
      "metadata": {
        "id": "fpz2O5Avom6N"
      },
      "id": "fpz2O5Avom6N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exported_ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "ckpt_manager = tf.train.CheckpointManager(\n",
        "exported_ckpt, directory=\"/content/new_config/checkpoint/\", max_to_keep=5)\n",
        "ckpt_manager.save()"
      ],
      "metadata": {
        "id": "F5sBjB2Sory_"
      },
      "id": "F5sBjB2Sory_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "08_object_detection_fine_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}